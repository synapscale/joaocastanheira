# Arquivo de Configuração para o Modelo de Linguagem (LLM) do Agente Principal
# Localização: src/agents/main_agent/agent.py (que carrega esta configuração)
# Usado por: Lógica de carregamento do modelo (ex: uma classe `ModelLoader` ou diretamente em `agent.py`)
#
# Este arquivo especifica qual modelo de linguagem o agente principal deve usar e como configurá-lo.
# A abordagem aqui prioriza o uso de variáveis de ambiente para dados sensíveis (API keys)
# e para parâmetros que podem mudar entre ambientes (dev, staging, prod).

# Provedor do modelo de linguagem.
# Opções comuns: "openai", "anthropic", "google", "huggingface_hub", "local_ollama", "custom_api".
# Esta configuração pode ser um valor padrão, mas idealmente a lógica de carregamento do modelo
# permitiria que fosse sobrescrita por uma variável de ambiente para máxima flexibilidade.
# Ex: PROVIDER_ENV_VAR: "LLM_PROVIDER"
provider: "openai"

# Nome da variável de ambiente que armazena o nome específico do modelo a ser usado.
# O valor desta variável de ambiente (ex: "gpt-4-turbo", "claude-3-opus-20240229") será lido em tempo de execução.
# Exemplo em .env: OPENAI_MODEL_NAME="gpt-4-turbo"
model_name_env_var: "OPENAI_MODEL_NAME"

# Nome da variável de ambiente que armazena a chave de API para o provedor do modelo.
# Exemplo em .env: OPENAI_API_KEY="sk-xxxxxxxxxxxx"
api_key_env_var: "OPENAI_API_KEY"

# Opcional: Nome da variável de ambiente para a URL base da API do modelo.
# Útil para proxies (ex: LiteLLM), instâncias auto-hospedadas (ex: Oobabooga com API OpenAI compatível),
# ou endpoints específicos de provedores (ex: Azure OpenAI).
# Exemplo em .env: OPENAI_API_BASE_URL="http://localhost:8000/v1"
# base_url_env_var: "OPENAI_API_BASE_URL"

# Nome da variável de ambiente para a temperatura padrão das respostas do modelo.
# A temperatura controla a aleatoriedade da saída do LLM.
# - Valores mais baixos (ex: 0.1, 0.2) tornam a saída mais determinística, focada e repetível.
# - Valores mais altos (ex: 0.7, 0.8, 1.0) tornam a saída mais criativa, diversa e às vezes surpreendente.
# Exemplo em .env: AGENT_DEFAULT_TEMPERATURE="0.7"
temperature_env_var: "AGENT_DEFAULT_TEMPERATURE"

# Nome da variável de ambiente para o número máximo padrão de tokens a serem gerados na resposta.
# Ajuda a controlar o custo e o tamanho da resposta. Deve ser ajustado conforme o modelo e a tarefa.
# Exemplo em .env: AGENT_DEFAULT_MAX_TOKENS="2048"
max_tokens_env_var: "AGENT_DEFAULT_MAX_TOKENS"

# Outros parâmetros específicos do modelo podem ser adicionados aqui.
# A lógica de carregamento do modelo em `agent.py` ou `model_loader.py` precisaria ser
# adaptada para ler e aplicar esses parâmetros adicionais.
#
# Exemplo para modelos OpenAI (os valores podem ser fixos ou também vir de env vars):
# openai_specific_params:
#   top_p: 1.0  # Controla a diversidade via amostragem de núcleo. 1.0 considera todos os tokens.
#   frequency_penalty: 0.0  # Penaliza tokens que já apareceram, reduzindo repetição. (Range: -2.0 a 2.0)
#   presence_penalty: 0.0   # Penaliza tokens novos, incentivando o modelo a falar sobre novos tópicos. (Range: -2.0 a 2.0)
#   # stop_sequences: ["\nUser:", "\nAssistant:"] # Sequências que, se geradas, param a geração.

# Estrutura alternativa (se não usar a convenção `_env_var` diretamente na lógica de carregamento):
# Se a lógica de carregamento do modelo for mais genérica e você preferir definir os nomes das env vars aqui:
# llm_parameters:
#   model_name:
#     env_var: "OPENAI_MODEL_NAME"
#     default: "gpt-3.5-turbo" # Um fallback se a env var não estiver definida
#   temperature:
#     env_var: "AGENT_DEFAULT_TEMPERATURE"
#     default: 0.7
#   max_tokens:
#     env_var: "AGENT_DEFAULT_MAX_TOKENS"
#     default: 1500
#   # ... outros parâmetros

# Considerações AI-Friendly:
# - Flexibilidade: Permitir que o modelo e seus parâmetros sejam configurados via variáveis de ambiente
#   facilita a experimentação e a adaptação a diferentes LLMs sem alterar o código.
# - Clareza: Documentar o propósito de cada parâmetro ajuda os usuários a entenderem como
#   ajustar o comportamento do LLM para suas necessidades específicas.
# - Padrões Sensatos: Fornecer valores padrão razoáveis (seja no código ou como comentários no .env.example)
#   ajuda a iniciar rapidamente.

